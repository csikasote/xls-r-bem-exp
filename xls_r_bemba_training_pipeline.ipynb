{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "xls-r-bemba-training-pipeline.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMKxweAcy0OKP9ZI3RRRFy3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e7fea84d0e524382b17259d48843cbbd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4ec62e69772a4cf8bb99c367b18fd107",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_590d59e769f74097afad9ab91ce2da46",
              "IPY_MODEL_e151794b583a49b6bfeb6d2699cb3750",
              "IPY_MODEL_8084e11f39494ef1a6139d0f81ddc42a",
              "IPY_MODEL_b454002691844d7e82e7d34515f4c151",
              "IPY_MODEL_1017f6f47adc4797869a68af2d377a9f"
            ]
          }
        },
        "4ec62e69772a4cf8bb99c367b18fd107": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "column",
            "width": "50%",
            "min_width": null,
            "border": null,
            "align_items": "center",
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "flex",
            "left": null
          }
        },
        "590d59e769f74097afad9ab91ce2da46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_298472cef9b340f58d753408dbba595f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "<center>\n<img src=https://huggingface.co/front/assets/huggingface_logo-noborder.svg alt='Hugging Face'>\n<br>\nCopy a token from <a href=\"https://huggingface.co/settings/token\" target=\"_blank\">your Hugging Face tokens page</a> and paste it below.\n<br>\nImmediately click login after copying your token or it might be stored in plain text in this notebook file.\n</center>",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7d573095b1a24c5087423cd9e452e735"
          }
        },
        "e151794b583a49b6bfeb6d2699cb3750": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "PasswordView",
            "style": "IPY_MODEL_bccb16e42b074dea8c7a8fbe53f4f052",
            "_dom_classes": [],
            "description": "Token:",
            "_model_name": "PasswordModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "",
            "_view_count": null,
            "disabled": false,
            "_view_module_version": "1.5.0",
            "continuous_update": true,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b2707110287147dabe5d1118353453de"
          }
        },
        "8084e11f39494ef1a6139d0f81ddc42a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ButtonView",
            "style": "IPY_MODEL_d99201836e4641ce9055dc061084af0c",
            "_dom_classes": [],
            "description": "Login",
            "_model_name": "ButtonModel",
            "button_style": "",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "tooltip": "",
            "_view_count": null,
            "disabled": false,
            "_view_module_version": "1.5.0",
            "layout": "IPY_MODEL_88edb13616fb4bec8fbfbbcaa4281609",
            "_model_module": "@jupyter-widgets/controls",
            "icon": ""
          }
        },
        "b454002691844d7e82e7d34515f4c151": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e50258bd58f742baadab7341c6957bb7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated 'notebooks' token with 'write' access, that you can then easily reuse for all notebooks.\n<br>\n<i>Logging in with your username and password is deprecated and won't be possible anymore in the near future. You can still use them for now by clicking below.</i>\n</center>",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_04c47deb0b6443f385e3730e5d835a9d"
          }
        },
        "1017f6f47adc4797869a68af2d377a9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ButtonView",
            "style": "IPY_MODEL_001f20c7cad24a63be578092b0a01f5f",
            "_dom_classes": [],
            "description": "Use password",
            "_model_name": "ButtonModel",
            "button_style": "",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "tooltip": "",
            "_view_count": null,
            "disabled": false,
            "_view_module_version": "1.5.0",
            "layout": "IPY_MODEL_44eccfc7e5324b2fb6c3823868b6abd9",
            "_model_module": "@jupyter-widgets/controls",
            "icon": ""
          }
        },
        "298472cef9b340f58d753408dbba595f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7d573095b1a24c5087423cd9e452e735": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bccb16e42b074dea8c7a8fbe53f4f052": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b2707110287147dabe5d1118353453de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d99201836e4641ce9055dc061084af0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ButtonStyleModel",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "button_color": null,
            "font_weight": "",
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "88edb13616fb4bec8fbfbbcaa4281609": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e50258bd58f742baadab7341c6957bb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "04c47deb0b6443f385e3730e5d835a9d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "001f20c7cad24a63be578092b0a01f5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ButtonStyleModel",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "button_color": null,
            "font_weight": "",
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "44eccfc7e5324b2fb6c3823868b6abd9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/csikasote/xls-r-bem-exp/blob/main/xls_r_bemba_training_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J46F6gTbFL-w",
        "outputId": "2f40d379-219b-497b-9385-339da353fad0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 311 kB 13.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 87.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 67 kB 5.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 243 kB 92.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 133 kB 84.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 71.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.8 MB 76.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 67.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 895 kB 83.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 50 kB 7.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 271 kB 71.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 144 kB 72.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 94 kB 4.2 MB/s \n",
            "\u001b[?25h  Building wheel for python-Levenshtein (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting https://github.com/kpu/kenlm/archive/master.zip\n",
            "  Downloading https://github.com/kpu/kenlm/archive/master.zip\n",
            "\u001b[K     / 541 kB 2.1 MB/s\n",
            "\u001b[?25hBuilding wheels for collected packages: kenlm\n",
            "  Building wheel for kenlm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kenlm: filename=kenlm-0.0.0-cp37-cp37m-linux_x86_64.whl size=2334664 sha256=d248620dd8227b11ef78faa56c0e23034ee851feef6aaa692a1fd6a06f492b8c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-6749vgpk/wheels/3d/aa/02/7b4a2eab5d7a2a9391bd9680dbad6270808a147bc3b7047e4e\n",
            "Successfully built kenlm\n",
            "Installing collected packages: kenlm\n",
            "Successfully installed kenlm-0.0.0\n",
            "Collecting pyctcdecode\n",
            "  Downloading pyctcdecode-0.3.0-py2.py3-none-any.whl (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 2.1 MB/s \n",
            "\u001b[?25hCollecting pygtrie<3.0,>=2.1\n",
            "  Downloading pygtrie-2.4.2.tar.gz (35 kB)\n",
            "Collecting hypothesis<7,>=6.14\n",
            "  Downloading hypothesis-6.37.0-py3-none-any.whl (377 kB)\n",
            "\u001b[K     |████████████████████████████████| 377 kB 30.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from pyctcdecode) (1.21.5)\n",
            "Requirement already satisfied: sortedcontainers<3.0.0,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from hypothesis<7,>=6.14->pyctcdecode) (2.4.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from hypothesis<7,>=6.14->pyctcdecode) (21.4.0)\n",
            "Building wheels for collected packages: pygtrie\n",
            "  Building wheel for pygtrie (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pygtrie: filename=pygtrie-2.4.2-py3-none-any.whl size=19063 sha256=900c4fc7f18c4b2d33df71a7fea4e282c8006fcea3841bc7e6acc5df4ca1342b\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/f8/ba/1d828b1603ea422686eb694253a43cb3a5901ea4696c1e0603\n",
            "Successfully built pygtrie\n",
            "Installing collected packages: pygtrie, hypothesis, pyctcdecode\n",
            "Successfully installed hypothesis-6.37.0 pyctcdecode-0.3.0 pygtrie-2.4.2\n"
          ]
        }
      ],
      "source": [
        "!pip -q install datasets transformers torchaudio librosa huggingface_hub jiwer\n",
        "!pip install https://github.com/kpu/kenlm/archive/master.zip\n",
        "!pip install pyctcdecode"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bitsandbytes-cuda111"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANEfCV50Goxg",
        "outputId": "1ce805e2-d44f-4888-c851-d63ea17762d8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bitsandbytes-cuda111\n",
            "  Downloading bitsandbytes_cuda111-0.26.0-py3-none-any.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 16.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: bitsandbytes-cuda111\n",
            "Successfully installed bitsandbytes-cuda111-0.26.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!apt install git-lfs"
      ],
      "metadata": {
        "id": "hWZB9wk-oHrH"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download BembaSpeech dataset from github repo\n",
        "!git clone https://github.com/csikasote/BembaSpeech.git \n",
        "%cd BembaSpeech\n",
        "!python prepare.py\n",
        "%cd /content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQwAqGPWFcHP",
        "outputId": "3696ff2b-4fec-4cca-c0ab-94ef1713c833"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'BembaSpeech'...\n",
            "remote: Enumerating objects: 35625, done.\u001b[K\n",
            "remote: Counting objects: 100% (242/242), done.\u001b[K\n",
            "remote: Compressing objects: 100% (239/239), done.\u001b[K\n",
            "remote: Total 35625 (delta 109), reused 7 (delta 1), pack-reused 35383\u001b[K\n",
            "Receiving objects: 100% (35625/35625), 2.44 GiB | 25.98 MiB/s, done.\n",
            "Resolving deltas: 100% (17681/17681), done.\n",
            "Checking out files: 100% (15140/15140), done.\n",
            "/content/BembaSpeech\n",
            "Preparing audio files and generating csv files for training the model ... \n",
            "\n",
            "N/A% (0 of 3) |                           | Elapsed Time: 0:00:00 ETA:  --:--:--> 'train.csv' has been processed successfully!\n",
            "> Training set size: 20.05 hrs\n",
            "> No of files: 11892\n",
            " 33% (1 of 3) |#########                  | Elapsed Time: 0:00:01 ETA:   0:00:02> 'dev.csv' has been created successfully!\n",
            "> Evaluation set size: 2.42 hrs\n",
            "> No of files: 1549\n",
            " 66% (2 of 3) |##################         | Elapsed Time: 0:00:02 ETA:   0:00:01> 'test.csv' has been created successfully!\n",
            "> Test set size: 2.05 hrs\n",
            "> No. of file: 977\n",
            "100% (3 of 3) |###########################| Elapsed Time: 0:00:02 Time:  0:00:02\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/huggingface/transformers/master/examples/research_projects/robust-speech-event/run_speech_recognition_ctc_bnb.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jL6KpIQ6FjZW",
        "outputId": "2201d4f1-3bc0-4cc3-dd8c-ace72627bff6"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-02-20 10:17:30--  https://raw.githubusercontent.com/huggingface/transformers/master/examples/research_projects/robust-speech-event/run_speech_recognition_ctc_bnb.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 31249 (31K) [text/plain]\n",
            "Saving to: ‘run_speech_recognition_ctc_bnb.py’\n",
            "\n",
            "\r          run_speec   0%[                    ]       0  --.-KB/s               \rrun_speech_recognit 100%[===================>]  30.52K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-02-20 10:17:30 (131 MB/s) - ‘run_speech_recognition_ctc_bnb.py’ saved [31249/31249]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387,
          "referenced_widgets": [
            "e7fea84d0e524382b17259d48843cbbd",
            "4ec62e69772a4cf8bb99c367b18fd107",
            "590d59e769f74097afad9ab91ce2da46",
            "e151794b583a49b6bfeb6d2699cb3750",
            "8084e11f39494ef1a6139d0f81ddc42a",
            "b454002691844d7e82e7d34515f4c151",
            "1017f6f47adc4797869a68af2d377a9f",
            "298472cef9b340f58d753408dbba595f",
            "7d573095b1a24c5087423cd9e452e735",
            "bccb16e42b074dea8c7a8fbe53f4f052",
            "b2707110287147dabe5d1118353453de",
            "d99201836e4641ce9055dc061084af0c",
            "88edb13616fb4bec8fbfbbcaa4281609",
            "e50258bd58f742baadab7341c6957bb7",
            "04c47deb0b6443f385e3730e5d835a9d",
            "001f20c7cad24a63be578092b0a01f5f",
            "44eccfc7e5324b2fb6c3823868b6abd9"
          ]
        },
        "id": "LHAsCwAnfUqg",
        "outputId": "047ce07f-632e-403d-9ece-7bba0f9637d7"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Login successful\n",
            "Your token has been saved to /root/.huggingface/token\n",
            "\u001b[1m\u001b[31mAuthenticated through git-credential store but this isn't the helper defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub. Run the following command in your terminal in case you want to set this credential helper as the default\n",
            "\n",
            "git config --global credential.helper store\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!echo '''python run_speech_recognition_ctc_bnb.py \\\n",
        "\t--dataset_name=\"BembaSpeech\" \\\n",
        "\t--model_name_or_path=\"facebook/wav2vec2-xls-r-1b\" \\\n",
        "\t--dataset_config_name=\"bem\" \\\n",
        "  --train_split_name=\"train\" \\\n",
        "  --train_split_path=\"/content/BembaSpeech/BembaSpeech/csv/train.csv\" \\\n",
        "  --eval_split_name=\"eval\" \\\n",
        "  --eval_split_path=\"/content/BembaSpeech/BembaSpeech/csv/dev.csv\" \\\n",
        "\t--output_dir=\"./xls-r-bem-test\" \\\n",
        "\t--overwrite_output_dir \\\n",
        "\t--max_steps=\"10\" \\\n",
        "\t--per_device_train_batch_size=\"2\" \\\n",
        "\t--learning_rate=\"3e-4\" \\\n",
        "\t--save_total_limit=\"1\" \\\n",
        "\t--evaluation_strategy=\"steps\" \\\n",
        "  --audio_column_name=\"audio\" \\\n",
        "\t--text_column_name=\"sentence\" \\\n",
        "\t--length_column_name=\"input_length\" \\\n",
        "\t--save_steps=\"5\" \\\n",
        "\t--layerdrop=\"0.0\" \\\n",
        "\t--freeze_feature_encoder \\\n",
        "\t--gradient_checkpointing \\\n",
        "\t--fp16 \\\n",
        "\t--group_by_length \\\n",
        "\t--push_to_hub \\\n",
        "\t--use_auth_token \\\n",
        "\t--do_train \\\n",
        "  --do_eval''' > run.sh"
      ],
      "metadata": {
        "id": "1nG3YtJXGXD0"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!bash run.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaff0NQfI_xk",
        "outputId": "a0fd4a8a-9b56-460f-bf96-da1f336ae828"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "02/20/2022 10:26:22 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
            "02/20/2022 10:26:22 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=500,\n",
            "evaluation_strategy=IntervalStrategy.STEPS,\n",
            "fp16=True,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=True,\n",
            "greater_is_better=None,\n",
            "group_by_length=True,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=0.0003,\n",
            "length_column_name=input_length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=./xls-r-bem-test/runs/Feb20_10-26-22_eff88612a38c,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=10,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=OptimizerNames.ADAMW_HF,\n",
            "output_dir=./xls-r-bem-test,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=2,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=True,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=./xls-r-bem-test,\n",
            "save_on_each_node=False,\n",
            "save_steps=5,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=1,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "02/20/2022 10:26:22 - WARNING - datasets.builder - Using custom data configuration default-a2087377799938e1\n",
            "02/20/2022 10:26:22 - WARNING - datasets.builder - Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-a2087377799938e1/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e)\n",
            "100% 2/2 [00:00<00:00, 869.29it/s]\n",
            "02/20/2022 10:26:22 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-a2087377799938e1/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-5781627b869aa6a0.arrow\n",
            "02/20/2022 10:26:22 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-a2087377799938e1/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-3ab4231a446706ee.arrow\n",
            "loading configuration file https://huggingface.co/facebook/wav2vec2-xls-r-1b/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/19f816c26d6fef49a4dfc0fc6b841c37792a250d2697d8432769f8af5698f1dc.90dd5f300087b6277c408283c36aefa2efb15afd0d3e210b3a7c3f3efc478d03\n",
            "Model config Wav2Vec2Config {\n",
            "  \"_name_or_path\": \"facebook/wav2vec2-xls-r-1b\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"adapter_kernel_size\": 3,\n",
            "  \"adapter_stride\": 2,\n",
            "  \"add_adapter\": false,\n",
            "  \"apply_spec_augment\": true,\n",
            "  \"architectures\": [\n",
            "    \"Wav2Vec2ForPreTraining\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"classifier_proj_size\": 256,\n",
            "  \"codevector_dim\": 1024,\n",
            "  \"contrastive_logits_temperature\": 0.1,\n",
            "  \"conv_bias\": true,\n",
            "  \"conv_dim\": [\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512\n",
            "  ],\n",
            "  \"conv_kernel\": [\n",
            "    10,\n",
            "    3,\n",
            "    3,\n",
            "    3,\n",
            "    3,\n",
            "    2,\n",
            "    2\n",
            "  ],\n",
            "  \"conv_stride\": [\n",
            "    5,\n",
            "    2,\n",
            "    2,\n",
            "    2,\n",
            "    2,\n",
            "    2,\n",
            "    2\n",
            "  ],\n",
            "  \"ctc_loss_reduction\": \"sum\",\n",
            "  \"ctc_zero_infinity\": false,\n",
            "  \"diversity_loss_weight\": 0.1,\n",
            "  \"do_stable_layer_norm\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"feat_extract_activation\": \"gelu\",\n",
            "  \"feat_extract_dropout\": 0.0,\n",
            "  \"feat_extract_norm\": \"layer\",\n",
            "  \"feat_proj_dropout\": 0.1,\n",
            "  \"feat_quantizer_dropout\": 0.0,\n",
            "  \"final_dropout\": 0.0,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout\": 0.1,\n",
            "  \"hidden_size\": 1280,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 5120,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"layerdrop\": 0.1,\n",
            "  \"mask_feature_length\": 10,\n",
            "  \"mask_feature_min_masks\": 0,\n",
            "  \"mask_feature_prob\": 0.0,\n",
            "  \"mask_time_length\": 10,\n",
            "  \"mask_time_min_masks\": 2,\n",
            "  \"mask_time_prob\": 0.075,\n",
            "  \"model_type\": \"wav2vec2\",\n",
            "  \"num_adapter_layers\": 3,\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_codevector_groups\": 2,\n",
            "  \"num_codevectors_per_group\": 320,\n",
            "  \"num_conv_pos_embedding_groups\": 16,\n",
            "  \"num_conv_pos_embeddings\": 128,\n",
            "  \"num_feat_extract_layers\": 7,\n",
            "  \"num_hidden_layers\": 48,\n",
            "  \"num_negatives\": 100,\n",
            "  \"output_hidden_size\": 1280,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"proj_codevector_dim\": 1024,\n",
            "  \"tdnn_dilation\": [\n",
            "    1,\n",
            "    2,\n",
            "    3,\n",
            "    1,\n",
            "    1\n",
            "  ],\n",
            "  \"tdnn_dim\": [\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    1500\n",
            "  ],\n",
            "  \"tdnn_kernel\": [\n",
            "    5,\n",
            "    3,\n",
            "    3,\n",
            "    1,\n",
            "    1\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.16.2\",\n",
            "  \"use_weighted_layer_sum\": false,\n",
            "  \"vocab_size\": 32,\n",
            "  \"xvector_output_dim\": 512\n",
            "}\n",
            "\n",
            "100% 1/1 [00:00<00:00, 25.28ba/s]\n",
            "100% 1/1 [00:00<00:00, 191.55ba/s]\n",
            "Didn't find file ./xls-r-bem-test/tokenizer_config.json. We won't load it.\n",
            "Didn't find file ./xls-r-bem-test/added_tokens.json. We won't load it.\n",
            "Didn't find file ./xls-r-bem-test/special_tokens_map.json. We won't load it.\n",
            "Didn't find file ./xls-r-bem-test/tokenizer.json. We won't load it.\n",
            "loading file ./xls-r-bem-test/vocab.json\n",
            "loading file None\n",
            "loading file None\n",
            "loading file None\n",
            "loading file None\n",
            "file ./xls-r-bem-test/config.json not found\n",
            "Adding <s> to the vocabulary\n",
            "Adding </s> to the vocabulary\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "loading configuration file https://huggingface.co/facebook/wav2vec2-xls-r-1b/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/19f816c26d6fef49a4dfc0fc6b841c37792a250d2697d8432769f8af5698f1dc.90dd5f300087b6277c408283c36aefa2efb15afd0d3e210b3a7c3f3efc478d03\n",
            "Model config Wav2Vec2Config {\n",
            "  \"_name_or_path\": \"facebook/wav2vec2-xls-r-1b\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"adapter_kernel_size\": 3,\n",
            "  \"adapter_stride\": 2,\n",
            "  \"add_adapter\": false,\n",
            "  \"apply_spec_augment\": true,\n",
            "  \"architectures\": [\n",
            "    \"Wav2Vec2ForPreTraining\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"classifier_proj_size\": 256,\n",
            "  \"codevector_dim\": 1024,\n",
            "  \"contrastive_logits_temperature\": 0.1,\n",
            "  \"conv_bias\": true,\n",
            "  \"conv_dim\": [\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512\n",
            "  ],\n",
            "  \"conv_kernel\": [\n",
            "    10,\n",
            "    3,\n",
            "    3,\n",
            "    3,\n",
            "    3,\n",
            "    2,\n",
            "    2\n",
            "  ],\n",
            "  \"conv_stride\": [\n",
            "    5,\n",
            "    2,\n",
            "    2,\n",
            "    2,\n",
            "    2,\n",
            "    2,\n",
            "    2\n",
            "  ],\n",
            "  \"ctc_loss_reduction\": \"sum\",\n",
            "  \"ctc_zero_infinity\": false,\n",
            "  \"diversity_loss_weight\": 0.1,\n",
            "  \"do_stable_layer_norm\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"feat_extract_activation\": \"gelu\",\n",
            "  \"feat_extract_dropout\": 0.0,\n",
            "  \"feat_extract_norm\": \"layer\",\n",
            "  \"feat_proj_dropout\": 0.1,\n",
            "  \"feat_quantizer_dropout\": 0.0,\n",
            "  \"final_dropout\": 0.0,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout\": 0.1,\n",
            "  \"hidden_size\": 1280,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 5120,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"layerdrop\": 0.1,\n",
            "  \"mask_feature_length\": 10,\n",
            "  \"mask_feature_min_masks\": 0,\n",
            "  \"mask_feature_prob\": 0.0,\n",
            "  \"mask_time_length\": 10,\n",
            "  \"mask_time_min_masks\": 2,\n",
            "  \"mask_time_prob\": 0.075,\n",
            "  \"model_type\": \"wav2vec2\",\n",
            "  \"num_adapter_layers\": 3,\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_codevector_groups\": 2,\n",
            "  \"num_codevectors_per_group\": 320,\n",
            "  \"num_conv_pos_embedding_groups\": 16,\n",
            "  \"num_conv_pos_embeddings\": 128,\n",
            "  \"num_feat_extract_layers\": 7,\n",
            "  \"num_hidden_layers\": 48,\n",
            "  \"num_negatives\": 100,\n",
            "  \"output_hidden_size\": 1280,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"proj_codevector_dim\": 1024,\n",
            "  \"tdnn_dilation\": [\n",
            "    1,\n",
            "    2,\n",
            "    3,\n",
            "    1,\n",
            "    1\n",
            "  ],\n",
            "  \"tdnn_dim\": [\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    1500\n",
            "  ],\n",
            "  \"tdnn_kernel\": [\n",
            "    5,\n",
            "    3,\n",
            "    3,\n",
            "    1,\n",
            "    1\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.16.2\",\n",
            "  \"use_weighted_layer_sum\": false,\n",
            "  \"vocab_size\": 32,\n",
            "  \"xvector_output_dim\": 512\n",
            "}\n",
            "\n",
            "loading feature extractor configuration file https://huggingface.co/facebook/wav2vec2-xls-r-1b/resolve/main/preprocessor_config.json from cache at /root/.cache/huggingface/transformers/a33f3a17d64af3dc7d6af9fe68e6fcbe153fd601b94de9e09ff27c6916beed02.d4484dc1c81456a2461485e7168b04347a7b9a4e3b1ef3aba723323b33e12326\n",
            "Feature extractor Wav2Vec2FeatureExtractor {\n",
            "  \"do_normalize\": true,\n",
            "  \"feature_extractor_type\": \"Wav2Vec2FeatureExtractor\",\n",
            "  \"feature_size\": 1,\n",
            "  \"padding_side\": \"right\",\n",
            "  \"padding_value\": 0,\n",
            "  \"return_attention_mask\": true,\n",
            "  \"sampling_rate\": 16000\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/facebook/wav2vec2-xls-r-1b/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/1b10cc2ca6452defe09996da7c9e2d4a9550e22295550c3659e8b1895c97921a.46e33ebecb25f07f270ccfa8896911dd7aeba6b646f8724d18117987926c98db\n",
            "Some weights of the model checkpoint at facebook/wav2vec2-xls-r-1b were not used when initializing Wav2Vec2ForCTC: ['project_q.weight', 'quantizer.weight_proj.weight', 'quantizer.weight_proj.bias', 'project_hid.bias', 'quantizer.codevectors', 'project_q.bias', 'project_hid.weight']\n",
            "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-xls-r-1b and are newly initialized: ['lm_head.bias', 'lm_head.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "preprocess datasets: 11892ex [00:37, 314.47ex/s]\n",
            "preprocess datasets: 1549ex [00:04, 381.98ex/s]\n",
            "100% 12/12 [00:00<00:00, 650.64ba/s]\n",
            "100% 2/2 [00:00<00:00, 622.25ba/s]\n",
            "Configuration saved in ./xls-r-bem-test/preprocessor_config.json\n",
            "tokenizer config file saved in ./xls-r-bem-test/tokenizer_config.json\n",
            "Special tokens file saved in ./xls-r-bem-test/special_tokens_map.json\n",
            "added tokens file saved in ./xls-r-bem-test/added_tokens.json\n",
            "Configuration saved in ./xls-r-bem-test/config.json\n",
            "loading feature extractor configuration file ./xls-r-bem-test/preprocessor_config.json\n",
            "loading configuration file ./xls-r-bem-test/config.json\n",
            "Model config Wav2Vec2Config {\n",
            "  \"_name_or_path\": \"./xls-r-bem-test\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"adapter_kernel_size\": 3,\n",
            "  \"adapter_stride\": 2,\n",
            "  \"add_adapter\": false,\n",
            "  \"apply_spec_augment\": true,\n",
            "  \"architectures\": [\n",
            "    \"Wav2Vec2ForPreTraining\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"classifier_proj_size\": 256,\n",
            "  \"codevector_dim\": 1024,\n",
            "  \"contrastive_logits_temperature\": 0.1,\n",
            "  \"conv_bias\": true,\n",
            "  \"conv_dim\": [\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512\n",
            "  ],\n",
            "  \"conv_kernel\": [\n",
            "    10,\n",
            "    3,\n",
            "    3,\n",
            "    3,\n",
            "    3,\n",
            "    2,\n",
            "    2\n",
            "  ],\n",
            "  \"conv_stride\": [\n",
            "    5,\n",
            "    2,\n",
            "    2,\n",
            "    2,\n",
            "    2,\n",
            "    2,\n",
            "    2\n",
            "  ],\n",
            "  \"ctc_loss_reduction\": \"mean\",\n",
            "  \"ctc_zero_infinity\": false,\n",
            "  \"diversity_loss_weight\": 0.1,\n",
            "  \"do_stable_layer_norm\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"feat_extract_activation\": \"gelu\",\n",
            "  \"feat_extract_dropout\": 0.0,\n",
            "  \"feat_extract_norm\": \"layer\",\n",
            "  \"feat_proj_dropout\": 0.0,\n",
            "  \"feat_quantizer_dropout\": 0.0,\n",
            "  \"final_dropout\": 0.0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout\": 0.0,\n",
            "  \"hidden_size\": 1280,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 5120,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"layerdrop\": 0.0,\n",
            "  \"mask_feature_length\": 10,\n",
            "  \"mask_feature_min_masks\": 0,\n",
            "  \"mask_feature_prob\": 0.0,\n",
            "  \"mask_time_length\": 10,\n",
            "  \"mask_time_min_masks\": 2,\n",
            "  \"mask_time_prob\": 0.05,\n",
            "  \"model_type\": \"wav2vec2\",\n",
            "  \"num_adapter_layers\": 3,\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_codevector_groups\": 2,\n",
            "  \"num_codevectors_per_group\": 320,\n",
            "  \"num_conv_pos_embedding_groups\": 16,\n",
            "  \"num_conv_pos_embeddings\": 128,\n",
            "  \"num_feat_extract_layers\": 7,\n",
            "  \"num_hidden_layers\": 48,\n",
            "  \"num_negatives\": 100,\n",
            "  \"output_hidden_size\": 1280,\n",
            "  \"pad_token_id\": 30,\n",
            "  \"proj_codevector_dim\": 1024,\n",
            "  \"tdnn_dilation\": [\n",
            "    1,\n",
            "    2,\n",
            "    3,\n",
            "    1,\n",
            "    1\n",
            "  ],\n",
            "  \"tdnn_dim\": [\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    1500\n",
            "  ],\n",
            "  \"tdnn_kernel\": [\n",
            "    5,\n",
            "    3,\n",
            "    3,\n",
            "    1,\n",
            "    1\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.16.2\",\n",
            "  \"use_weighted_layer_sum\": false,\n",
            "  \"vocab_size\": 33,\n",
            "  \"xvector_output_dim\": 512\n",
            "}\n",
            "\n",
            "loading feature extractor configuration file ./xls-r-bem-test/preprocessor_config.json\n",
            "Feature extractor Wav2Vec2FeatureExtractor {\n",
            "  \"do_normalize\": true,\n",
            "  \"feature_extractor_type\": \"Wav2Vec2FeatureExtractor\",\n",
            "  \"feature_size\": 1,\n",
            "  \"padding_side\": \"right\",\n",
            "  \"padding_value\": 0,\n",
            "  \"return_attention_mask\": true,\n",
            "  \"sampling_rate\": 16000\n",
            "}\n",
            "\n",
            "Didn't find file ./xls-r-bem-test/tokenizer.json. We won't load it.\n",
            "loading file ./xls-r-bem-test/vocab.json\n",
            "loading file ./xls-r-bem-test/tokenizer_config.json\n",
            "loading file ./xls-r-bem-test/added_tokens.json\n",
            "loading file ./xls-r-bem-test/special_tokens_map.json\n",
            "loading file None\n",
            "Adding <s> to the vocabulary\n",
            "Adding </s> to the vocabulary\n",
            "Cloning https://huggingface.co/csikasote/xls-r-bem-test into local empty directory.\n",
            "02/20/2022 10:27:29 - WARNING - huggingface_hub.repository - Cloning https://huggingface.co/csikasote/xls-r-bem-test into local empty directory.\n",
            "max_steps is given, it will override any value given in num_train_epochs\n",
            "Using amp half precision backend\n",
            "The following columns in the training set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length.\n",
            "***** Running training *****\n",
            "  Num examples = 11892\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 2\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 10\n",
            " 50% 5/10 [00:04<00:03,  1.39it/s]Saving model checkpoint to ./xls-r-bem-test/checkpoint-5\n",
            "Configuration saved in ./xls-r-bem-test/checkpoint-5/config.json\n",
            "Model weights saved in ./xls-r-bem-test/checkpoint-5/pytorch_model.bin\n",
            "Configuration saved in ./xls-r-bem-test/checkpoint-5/preprocessor_config.json\n",
            "Configuration saved in ./xls-r-bem-test/preprocessor_config.json\n",
            "100% 10/10 [01:13<00:00,  5.52s/it]Saving model checkpoint to ./xls-r-bem-test/checkpoint-10\n",
            "Configuration saved in ./xls-r-bem-test/checkpoint-10/config.json\n",
            "Model weights saved in ./xls-r-bem-test/checkpoint-10/pytorch_model.bin\n",
            "Configuration saved in ./xls-r-bem-test/checkpoint-10/preprocessor_config.json\n",
            "Deleting older checkpoint [xls-r-bem-test/checkpoint-5] due to args.save_total_limit\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 95.4675, 'train_samples_per_second': 0.209, 'train_steps_per_second': 0.105, 'train_loss': 9.368547058105468, 'epoch': 0.0}\n",
            "100% 10/10 [01:35<00:00,  9.54s/it]\n",
            "Saving model checkpoint to ./xls-r-bem-test\n",
            "Configuration saved in ./xls-r-bem-test/config.json\n",
            "Model weights saved in ./xls-r-bem-test/pytorch_model.bin\n",
            "Configuration saved in ./xls-r-bem-test/preprocessor_config.json\n",
            "Saving model checkpoint to ./xls-r-bem-test\n",
            "Configuration saved in ./xls-r-bem-test/config.json\n",
            "Model weights saved in ./xls-r-bem-test/pytorch_model.bin\n",
            "Configuration saved in ./xls-r-bem-test/preprocessor_config.json\n",
            "Several commits (2) will be pushed upstream.\n",
            "02/20/2022 10:33:18 - WARNING - huggingface_hub.repository - Several commits (2) will be pushed upstream.\n",
            "The progress bars may be unreliable.\n",
            "02/20/2022 10:33:18 - WARNING - huggingface_hub.repository - The progress bars may be unreliable.\n",
            "Upload file pytorch_model.bin:   0% 3.39k/3.59G [00:00<?, ?B/s]\n",
            "Upload file pytorch_model.bin:  50% 1.79G/3.59G [51:03<01:29, 21.6MB/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "FV9lNrVAorTP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}